{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import RSLPStemmer\n",
    "\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_keywords = [\"feliz\",\n",
    "                     \"amor\",\n",
    "                     \"obrigado OR obrigada\",\n",
    "                     \"√≥timo OR √≥tima\",\n",
    "                     \"parab√©ns\",\n",
    "                     \"fant√°stico OR fant√°stica\", \n",
    "                     \"maravilha OR maravilhoso OR maravilhosa\"]\n",
    "            \n",
    "                     \n",
    "negative_keywords = [\"fml\",\n",
    "                     \"p√©ssimo OR p√©ssima\",\n",
    "                     \"tr√°gico OR tr√°gica\",\n",
    "                     \"horr√≠vel\",\n",
    "                     \"mau OR m√°\",\n",
    "                     \"terr√≠vel\", \n",
    "                     \"detesto OR detestei\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=[\"id\", \"created_at\", \"tweet\", \"keyword\", \"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>tweet</th>\n",
       "      <th>keyword</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [id, created_at, tweet, keyword, target]\n",
       "Index: []"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for keyword in positive_keywords:\n",
    "    temp_df = pd.read_csv(f\"data/raw_tweets_{keyword}.csv\", header=None)\n",
    "    temp_df.columns = [\"id\", \"created_at\", \"tweet\"]\n",
    "    temp_df[\"keyword\"] = [keyword] * len(temp_df)\n",
    "    temp_df[\"target\"] = [\"positive\"] * len(temp_df)\n",
    "    df = df.append(temp_df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for keyword in negative_keywords:\n",
    "    temp_df = pd.read_csv(f\"data/raw_tweets_{keyword}.csv\", header=None)\n",
    "    temp_df.columns = [\"id\", \"created_at\", \"tweet\"]\n",
    "    temp_df[\"keyword\"] = [keyword] * len(temp_df)\n",
    "    temp_df[\"target\"] = [\"negative\"] * len(temp_df)\n",
    "    df = df.append(temp_df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>tweet</th>\n",
       "      <th>keyword</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1396047477695029249</td>\n",
       "      <td>2021-05-22 10:17:10+00:00</td>\n",
       "      <td>Tava t√£o feliz c o apartamento mas acho q √© golpe</td>\n",
       "      <td>feliz</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1396047411047542785</td>\n",
       "      <td>2021-05-22 10:16:54+00:00</td>\n",
       "      <td>@rita_castro1 Bom dia Sweetie!! S√°bado feliz!!...</td>\n",
       "      <td>feliz</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1396047195921604611</td>\n",
       "      <td>2021-05-22 10:16:03+00:00</td>\n",
       "      <td>Bom dia e um feliz s√°bado a todos ‚úåüèºüíúüçÄ. üòòüòò htt...</td>\n",
       "      <td>feliz</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1396046918153904128</td>\n",
       "      <td>2021-05-22 10:14:57+00:00</td>\n",
       "      <td>Eu estou t√£o feliz pela Hande ela merece tudo !</td>\n",
       "      <td>feliz</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1396045926016368642</td>\n",
       "      <td>2021-05-22 10:11:00+00:00</td>\n",
       "      <td>Estou tao feliz finalmente em Castelo Branco c...</td>\n",
       "      <td>feliz</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23327</th>\n",
       "      <td>1397872771640827908</td>\n",
       "      <td>2021-05-27 11:10:14+00:00</td>\n",
       "      <td>Eu: detesto musicais ü§Æü§Æü§Æü§Æ\\n\\nAlso eu a dois mi...</td>\n",
       "      <td>detesto OR detestei</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23328</th>\n",
       "      <td>1397867369276579840</td>\n",
       "      <td>2021-05-27 10:48:46+00:00</td>\n",
       "      <td>Detesto est√° situa√ß√£o poha</td>\n",
       "      <td>detesto OR detestei</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23329</th>\n",
       "      <td>1397839222883688449</td>\n",
       "      <td>2021-05-27 08:56:55+00:00</td>\n",
       "      <td>Que linda noite de sono ao sonhar com a pessoa...</td>\n",
       "      <td>detesto OR detestei</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23330</th>\n",
       "      <td>1397833381099061248</td>\n",
       "      <td>2021-05-27 08:33:43+00:00</td>\n",
       "      <td>@Joaohpr Tamb√©m detesto e evito sempre que exi...</td>\n",
       "      <td>detesto OR detestei</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23331</th>\n",
       "      <td>1397823272058855425</td>\n",
       "      <td>2021-05-27 07:53:33+00:00</td>\n",
       "      <td>Eu adoro roupa, adoro moda.. Mas trabalhar num...</td>\n",
       "      <td>detesto OR detestei</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23332 rows √ó 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        id                 created_at  \\\n",
       "0      1396047477695029249  2021-05-22 10:17:10+00:00   \n",
       "1      1396047411047542785  2021-05-22 10:16:54+00:00   \n",
       "2      1396047195921604611  2021-05-22 10:16:03+00:00   \n",
       "3      1396046918153904128  2021-05-22 10:14:57+00:00   \n",
       "4      1396045926016368642  2021-05-22 10:11:00+00:00   \n",
       "...                    ...                        ...   \n",
       "23327  1397872771640827908  2021-05-27 11:10:14+00:00   \n",
       "23328  1397867369276579840  2021-05-27 10:48:46+00:00   \n",
       "23329  1397839222883688449  2021-05-27 08:56:55+00:00   \n",
       "23330  1397833381099061248  2021-05-27 08:33:43+00:00   \n",
       "23331  1397823272058855425  2021-05-27 07:53:33+00:00   \n",
       "\n",
       "                                                   tweet              keyword  \\\n",
       "0      Tava t√£o feliz c o apartamento mas acho q √© golpe                feliz   \n",
       "1      @rita_castro1 Bom dia Sweetie!! S√°bado feliz!!...                feliz   \n",
       "2      Bom dia e um feliz s√°bado a todos ‚úåüèºüíúüçÄ. üòòüòò htt...                feliz   \n",
       "3        Eu estou t√£o feliz pela Hande ela merece tudo !                feliz   \n",
       "4      Estou tao feliz finalmente em Castelo Branco c...                feliz   \n",
       "...                                                  ...                  ...   \n",
       "23327  Eu: detesto musicais ü§Æü§Æü§Æü§Æ\\n\\nAlso eu a dois mi...  detesto OR detestei   \n",
       "23328                         Detesto est√° situa√ß√£o poha  detesto OR detestei   \n",
       "23329  Que linda noite de sono ao sonhar com a pessoa...  detesto OR detestei   \n",
       "23330  @Joaohpr Tamb√©m detesto e evito sempre que exi...  detesto OR detestei   \n",
       "23331  Eu adoro roupa, adoro moda.. Mas trabalhar num...  detesto OR detestei   \n",
       "\n",
       "         target  \n",
       "0      positive  \n",
       "1      positive  \n",
       "2      positive  \n",
       "3      positive  \n",
       "4      positive  \n",
       "...         ...  \n",
       "23327  negative  \n",
       "23328  negative  \n",
       "23329  negative  \n",
       "23330  negative  \n",
       "23331  negative  \n",
       "\n",
       "[23332 rows x 5 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23282    @marekjusk @inesmorsantos Longe disso!! Detest...\n",
       "23283    @ruiclassic detesto o pizzi eu, mas tu es dema...\n",
       "23284    @MiguelBushes Eu j√° detesto aquela comiss√£o de...\n",
       "23285    @AnaPRDeepBlue Ainda agora li da variante Indi...\n",
       "23286    @_V_Castello üòÇüòÇ\\nNormalmente, detesto ter raz√£...\n",
       "23287    Detesto quando combino uma hora e a pessoa che...\n",
       "23288    @nunocalvin Pade√ßo do oposto em Lisboa e detes...\n",
       "23289         Detesto q fodam as merdas q tenho combinadas\n",
       "23290    @offcrissy Detesto isso pq o humor negro (o \"v...\n",
       "23291                 Detesto ter q tomar estas decis√µes üò•\n",
       "23292    @dianarrmiranda eu vou na 6a, houve um episodi...\n",
       "23293         Juro, detesto pessoal gemado no Clash Royale\n",
       "23294    Detesto conversar com gente que conta v√°rias s...\n",
       "23295                    Est√° um calor do caralho, detesto\n",
       "23296    E gostava de dizer o seguinte: O IMI n√£o devia...\n",
       "23297    Dois pidezinhos de esquerda bloqueados logo de...\n",
       "23298                      J√° disse que detesto ortopedia?\n",
       "23299    @nocas_miranda √â mesmo enervante n√£o √©? Detest...\n",
       "23300                     Detesto discutir logo de manh√£ üò°\n",
       "23301     Detesto pessoas que n√£o sabem aquilo que querem.\n",
       "23302    \"Detesto essas pessoas que te olham de cima a ...\n",
       "23303    Eu simplesmente detesto o facto das denomina√ß√µ...\n",
       "23304    Detesto mas detesto mesmo que demorem a responder\n",
       "23305    Detesto quando dizem algo meio for√ßado e eu re...\n",
       "23306    @Syrah___ Essa mulher para al√©m de ter sempre ...\n",
       "23307    @kariasMusic Pior √© que depois detesto que dei...\n",
       "23308    @andreiantunes_ pior que √© verdade, eu detesto...\n",
       "23309    Detesto fazer frequ√™ncias e depois ainda detes...\n",
       "23310    detesto trabalhos de grupo fr, no shade mas er...\n",
       "23311    Detesto que me digam para ter calma quando est...\n",
       "23312                Detesto estar a sentir-me assim fdsss\n",
       "23313    Detestei a aula de hoje. Foi das piores desde ...\n",
       "23314     Mano Fds detesto me sentir a mais digam logo Fds\n",
       "23315    Nngm entende o quanto eu detesto estar doente ...\n",
       "23316    Detesto sonhar com trai√ß√µes.. Sinto me mesmo m...\n",
       "23317    sempre a mesma merda. Detesto que n√£o se acred...\n",
       "23318           Detesto que se metam na minha rela√ß√£o fdss\n",
       "23319                                 Detesto aquele mi√∫do\n",
       "23320                               Detesto ter enxaquecas\n",
       "23321    Sempre detestei a criatura, sem raz√£o aparente...\n",
       "23322           Detesto q me ignorem quando digo boa tarde\n",
       "23323    Detesto que me d√™em vista , quando √© assim n√£o...\n",
       "23324    detesto o spotify no telem√≥vel. meto as playli...\n",
       "23325    Detesto sentir que estou a ser chata ou a inco...\n",
       "23326    Sabem aquela vontade de ir trabalhar?\\nPois......\n",
       "23327    Eu: detesto musicais ü§Æü§Æü§Æü§Æ\\n\\nAlso eu a dois mi...\n",
       "23328                           Detesto est√° situa√ß√£o poha\n",
       "23329    Que linda noite de sono ao sonhar com a pessoa...\n",
       "23330    @Joaohpr Tamb√©m detesto e evito sempre que exi...\n",
       "23331    Eu adoro roupa, adoro moda.. Mas trabalhar num...\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"tweet\"].tail(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some tweets may contain more than one keyword, and can therefore appear more than once in the dataset; this is not desirable, \n",
    "# so I will remove them\n",
    "\n",
    "df = df.drop_duplicates(subset=\"id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning - functions\n",
    "These functions were also saved to a [.py file](data_cleaning_functions.py) that can be imported in the next steps of training and validating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@TelmaCarlos7 vamos roubar na melicia, tudo p te fazer feliz\n",
      "@telmacarlos7 vamos roubar na melicia, tudo p te fazer feliz\n"
     ]
    }
   ],
   "source": [
    "def remove_pt_special_chars(text):\n",
    "    '''Replaces portuguese special characters (accented vowels and √ß) with their base cognates. Input should be lower case.'''\n",
    "    text = re.sub(r\"[√†√°√£√¢]\", \"a\", text)\n",
    "    text = re.sub(r\"[√©√™]\", \"e\", text)\n",
    "    text = re.sub(r\"[√≠]\", \"i\", text)\n",
    "    text = re.sub(r\"[√≥√¥√µ]\", \"o\", text)\n",
    "    text = re.sub(r\"[√∫]\", \"u\", text)\n",
    "    text = re.sub(r\"[√ß]\", \"c\", text)\n",
    "    return(text)\n",
    "\n",
    "print(df.loc[19, \"tweet\"])\n",
    "print(remove_pt_special_chars(df.loc[19, \"tweet\"].lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@TelmaCarlos7 vamos roubar na melicia, tudo p te fazer feliz\n",
      " vamos roubar na melicia, tudo p te fazer feliz\n"
     ]
    }
   ],
   "source": [
    "def remove_mentions(text):\n",
    "    '''Removes mentions to other twitter users.'''\n",
    "    text = re.sub(r\"(@[^ !]*)\", \"\", text)\n",
    "    return(text)\n",
    "\n",
    "print(df.loc[19, \"tweet\"])\n",
    "print(remove_mentions(df.loc[19, \"tweet\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@rita_castro1 Bom dia Sweetie!! S√°bado feliz!! üåûüôèü•∞‚òïüòò\n",
      "@rita_castro1 Bom dia Sweetie!! S√°bado feliz!! üåûüôèü•∞‚òïüòò\n"
     ]
    }
   ],
   "source": [
    "def remove_links(text):\n",
    "    '''Removes links from text.'''\n",
    "    text = re.sub(r\"http[\\S]*\", \"\", text)\n",
    "    text = re.sub(r\"www.[\\S]*\", \"\", text)\n",
    "    return(text)\n",
    "\n",
    "print(df.loc[1, \"tweet\"])\n",
    "print(remove_links(df.loc[1, \"tweet\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~‚Äú‚Äù]'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"[{punctuation[1:]}‚Äú‚Äù]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@Maraia21401743 Obrigada Maraia!ü§ó‚ô•Ô∏è\n",
      "Feliz dia,bom fim de semana!üòòüåºüåπ\n",
      "#CanYaman\n",
      "#SandokanTheSeries\n",
      "#Sandokan\n",
      "#LuxVide https://t.co/uixkrq949k\n",
      " Maraia21401743 Obrigada Maraia ! ü§ó‚ô•Ô∏è Feliz dia bom fim de semana ! üòòüåºüåπ CanYaman SandokanTheSeries Sandokan LuxVide https t co uixkrq949k\n"
     ]
    }
   ],
   "source": [
    "def clean_punctuation_and_newlines(text):\n",
    "    '''Removes punctuation *except* exclamation marks. Reduces multiple exclamation marks to a single one and adds a space \n",
    "    between words and exclamation mark. Also removes newline characters and reduces multiple spaces to one'''\n",
    "    text = re.sub(f\"[{punctuation[1:]}‚Äú‚Äù]\", \" \", text)\n",
    "    text = re.sub(r\"\\n\", \" \", text)\n",
    "    text = re.sub(r\"(!+)\", \" ! \", text)\n",
    "    text = re.sub(r\" +\", \" \", text)\n",
    "    return(text)\n",
    "\n",
    "print(df.loc[5, \"tweet\"])\n",
    "print(clean_punctuation_and_newlines(df.loc[5, \"tweet\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@r_rodrigues0303 @0705Rita fico feliz por ouvir isso\n",
      "    fico feliz por ouvir isso\n"
     ]
    }
   ],
   "source": [
    "def remove_numbers(text):\n",
    "    '''Removes numbers and words containing numbers'''\n",
    "    text = re.sub(r\"\\S*[0-9]+\\S*\", \" \", text)\n",
    "    return(text)\n",
    "\n",
    "print(df.loc[17, \"tweet\"])\n",
    "print(remove_numbers(df.loc[17, \"tweet\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@rita_castro1 Bom dia Sweetie!! S√°bado feliz!! üåûüôèü•∞‚òïüòò\n",
      "@rita_castro1 Bom dia Sweetie!! S√°bado feliz!! ‚òï\n"
     ]
    }
   ],
   "source": [
    "def remove_emojis(text):\n",
    "    '''Removes most emojis from the text; some are still left.'''\n",
    "    RE_EMOJI = re.compile('[\\U00010000-\\U0010ffff]', flags=re.UNICODE)\n",
    "    return RE_EMOJI.sub(r'', text)\n",
    "\n",
    "print(df.loc[1, \"tweet\"])\n",
    "print(remove_emojis(df.loc[1, \"tweet\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feliz aniversario aaaaaaa vem ca amorr\n",
      "feliz aniversario a vem ca amor\n",
      "\n",
      "oh meu deeeeusss isso e fanttaaastticooo adoorroooo arrre\n",
      "oh meu deus isso e fantastico adorro arre\n"
     ]
    }
   ],
   "source": [
    "def remove_repeated_letters(text):\n",
    "    '''Reduces repeated letters except r or s anywhere in each word by a single letter. Repeated r or s inside words are \n",
    "    reduced to rr or ss respectively, to account for words where this naturally occurs. Repeated r or s at the end of words are\n",
    "    reduced to single letters.\n",
    "    '''\n",
    "    \n",
    "    clean_text = []\n",
    "    for i in text.split():\n",
    "        word = re.sub(r\"([a-z])\\1{1,}$\", r\"\\1\", i)         #replaces repeated letters at the end of the word with single letter\n",
    "        word = re.sub(r\"([^sr])\\1{1,}(?=.)\", r\"\\1\", word)  #replaces repeated letters (except r or s) anywhere with single letter\n",
    "        word = re.sub(r\"([rs])\\1{1,}(?=.)\", r\"\\1\\1\", word) #replaces repeated internal r or s with double r or s\n",
    "        clean_text.append(word)\n",
    "    return(\" \".join(clean_text))\n",
    "\n",
    "print(\"feliz aniversario aaaaaaa vem ca amorr\")\n",
    "print(remove_repeated_letters(\"feliz aniversario aaaaaaa vem ca amorr\"))\n",
    "print(\"\")\n",
    "print(\"oh meu deeeeusss isso e fanttaaastticooo adoorroooo arrre\")\n",
    "print(remove_repeated_letters(\"oh meu deeeeusss isso e fanttaaastticooo adoorroooo arrre\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benzema: Estar na sele√ß√£o √© uma recompensa e estou super feliz https://t.co/zY6FYKd266\n",
      "benzema: est na sele√ß √© uma recompens e est sup feliz https://t.co/zy6fykd266\n"
     ]
    }
   ],
   "source": [
    "def apply_stemming(text):    \n",
    "    '''Applies stemming using the Porter stemmer implementation for portuguese contained in the nltk library.\n",
    "    Still not sure if it's worth using this or not, because the results are... sketchy, to say the least.\n",
    "    '''\n",
    "    stemmer = RSLPStemmer()\n",
    "    return(\" \".join([stemmer.stem(i) for i in text.split()]))\n",
    "\n",
    "print(df.loc[8, \"tweet\"])\n",
    "print(apply_stemming(df.loc[8, \"tweet\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tava t√£o feliz c o apartamento mas acho q √© golpe\n",
      "tav tao feliz c o apart mas ach q e golp\n"
     ]
    }
   ],
   "source": [
    "def clean_up_tweets(tweet):\n",
    "    clean_tweet = tweet.lower()\n",
    "    clean_tweet = remove_mentions(clean_tweet)\n",
    "    clean_tweet = remove_links(clean_tweet)\n",
    "    clean_tweet = remove_numbers(clean_tweet)\n",
    "    clean_tweet = remove_emojis(clean_tweet)\n",
    "    clean_tweet = clean_punctuation_and_newlines(clean_tweet)\n",
    "    clean_tweet = apply_stemming(clean_tweet)\n",
    "    clean_tweet = remove_pt_special_chars(clean_tweet) \n",
    "    clean_tweet = remove_repeated_letters(clean_tweet)\n",
    "\n",
    "    return(clean_tweet.strip(\" \"))\n",
    "\n",
    "print(df.loc[0, \"tweet\"])\n",
    "print(clean_up_tweets(df.loc[0, \"tweet\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "processed_stopwords = [clean_up_tweets(i) for i in stopwords.words(\"portuguese\")] + [\"q\", \"k\", \"c\", \"p\"]\n",
    "processed_stopwords.remove(\"nao\")\n",
    "\n",
    "processed_keywords = []\n",
    "\n",
    "for i in positive_keywords + negative_keywords:\n",
    "    for j in i.split():\n",
    "        if j != \"OR\":\n",
    "            processed_keywords.append(clean_up_tweets(j))\n",
    "\n",
    "processed_stopwords = list(set(processed_stopwords + processed_keywords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tava t√£o feliz c o apartamento mas acho q √© golpe\n",
      "tav tao feliz c o apart mas ach q e golp\n",
      "tav tao apart ach golp\n"
     ]
    }
   ],
   "source": [
    "def remove_stopwords(text, stopwords):\n",
    "    return(\" \".join([i for i in text.split() if i not in stopwords]))\n",
    "\n",
    "print(df.loc[0,\"tweet\"])\n",
    "print(clean_up_tweets(df.loc[0,\"tweet\"]))\n",
    "print(remove_stopwords(clean_up_tweets(df.loc[0,\"tweet\"]), processed_stopwords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning up the tweets\n",
    "Here I apply all data cleaning functions written above to my corpus of raw tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp/ipykernel_5592/1985238246.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"clean_tweet\"] = df[\"tweet\"].apply(lambda x: remove_stopwords(clean_up_tweets(x), processed_stopwords))\n"
     ]
    }
   ],
   "source": [
    "df[\"clean_tweet\"] = df[\"tweet\"].apply(lambda x: remove_stopwords(clean_up_tweets(x), processed_stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>tweet</th>\n",
       "      <th>keyword</th>\n",
       "      <th>target</th>\n",
       "      <th>clean_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1396047477695029249</td>\n",
       "      <td>2021-05-22 10:17:10+00:00</td>\n",
       "      <td>Tava t√£o feliz c o apartamento mas acho q √© golpe</td>\n",
       "      <td>feliz</td>\n",
       "      <td>positive</td>\n",
       "      <td>tav tao apart ach golp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1396047411047542785</td>\n",
       "      <td>2021-05-22 10:16:54+00:00</td>\n",
       "      <td>@rita_castro1 Bom dia Sweetie!! S√°bado feliz!!...</td>\n",
       "      <td>feliz</td>\n",
       "      <td>positive</td>\n",
       "      <td>bom dia sweti ! sab ! ‚òï</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1396047195921604611</td>\n",
       "      <td>2021-05-22 10:16:03+00:00</td>\n",
       "      <td>Bom dia e um feliz s√°bado a todos ‚úåüèºüíúüçÄ. üòòüòò htt...</td>\n",
       "      <td>feliz</td>\n",
       "      <td>positive</td>\n",
       "      <td>bom dia sab tod ‚úå</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1396046918153904128</td>\n",
       "      <td>2021-05-22 10:14:57+00:00</td>\n",
       "      <td>Eu estou t√£o feliz pela Hande ela merece tudo !</td>\n",
       "      <td>feliz</td>\n",
       "      <td>positive</td>\n",
       "      <td>tao hand merec tud !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1396045926016368642</td>\n",
       "      <td>2021-05-22 10:11:00+00:00</td>\n",
       "      <td>Estou tao feliz finalmente em Castelo Branco c...</td>\n",
       "      <td>feliz</td>\n",
       "      <td>positive</td>\n",
       "      <td>tao final castel branc xuxu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23327</th>\n",
       "      <td>1397872771640827908</td>\n",
       "      <td>2021-05-27 11:10:14+00:00</td>\n",
       "      <td>Eu: detesto musicais ü§Æü§Æü§Æü§Æ\\n\\nAlso eu a dois mi...</td>\n",
       "      <td>detesto OR detestei</td>\n",
       "      <td>negative</td>\n",
       "      <td>music als doi minut episodi music anatom grey ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23328</th>\n",
       "      <td>1397867369276579840</td>\n",
       "      <td>2021-05-27 10:48:46+00:00</td>\n",
       "      <td>Detesto est√° situa√ß√£o poha</td>\n",
       "      <td>detesto OR detestei</td>\n",
       "      <td>negative</td>\n",
       "      <td>situ poh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23329</th>\n",
       "      <td>1397839222883688449</td>\n",
       "      <td>2021-05-27 08:56:55+00:00</td>\n",
       "      <td>Que linda noite de sono ao sonhar com a pessoa...</td>\n",
       "      <td>detesto OR detestei</td>\n",
       "      <td>negative</td>\n",
       "      <td>lind noit son sonh pesso conhec faculdad ent ta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23330</th>\n",
       "      <td>1397833381099061248</td>\n",
       "      <td>2021-05-27 08:33:43+00:00</td>\n",
       "      <td>@Joaohpr Tamb√©m detesto e evito sempre que exi...</td>\n",
       "      <td>detesto OR detestei</td>\n",
       "      <td>negative</td>\n",
       "      <td>evit sempr exist altern fac tap ryana ra tem‚Ä¶</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23331</th>\n",
       "      <td>1397823272058855425</td>\n",
       "      <td>2021-05-27 07:53:33+00:00</td>\n",
       "      <td>Eu adoro roupa, adoro moda.. Mas trabalhar num...</td>\n",
       "      <td>detesto OR detestei</td>\n",
       "      <td>negative</td>\n",
       "      <td>ador roup ador mod trabalh loj roup lid pesso ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22842 rows √ó 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        id                 created_at  \\\n",
       "0      1396047477695029249  2021-05-22 10:17:10+00:00   \n",
       "1      1396047411047542785  2021-05-22 10:16:54+00:00   \n",
       "2      1396047195921604611  2021-05-22 10:16:03+00:00   \n",
       "3      1396046918153904128  2021-05-22 10:14:57+00:00   \n",
       "4      1396045926016368642  2021-05-22 10:11:00+00:00   \n",
       "...                    ...                        ...   \n",
       "23327  1397872771640827908  2021-05-27 11:10:14+00:00   \n",
       "23328  1397867369276579840  2021-05-27 10:48:46+00:00   \n",
       "23329  1397839222883688449  2021-05-27 08:56:55+00:00   \n",
       "23330  1397833381099061248  2021-05-27 08:33:43+00:00   \n",
       "23331  1397823272058855425  2021-05-27 07:53:33+00:00   \n",
       "\n",
       "                                                   tweet              keyword  \\\n",
       "0      Tava t√£o feliz c o apartamento mas acho q √© golpe                feliz   \n",
       "1      @rita_castro1 Bom dia Sweetie!! S√°bado feliz!!...                feliz   \n",
       "2      Bom dia e um feliz s√°bado a todos ‚úåüèºüíúüçÄ. üòòüòò htt...                feliz   \n",
       "3        Eu estou t√£o feliz pela Hande ela merece tudo !                feliz   \n",
       "4      Estou tao feliz finalmente em Castelo Branco c...                feliz   \n",
       "...                                                  ...                  ...   \n",
       "23327  Eu: detesto musicais ü§Æü§Æü§Æü§Æ\\n\\nAlso eu a dois mi...  detesto OR detestei   \n",
       "23328                         Detesto est√° situa√ß√£o poha  detesto OR detestei   \n",
       "23329  Que linda noite de sono ao sonhar com a pessoa...  detesto OR detestei   \n",
       "23330  @Joaohpr Tamb√©m detesto e evito sempre que exi...  detesto OR detestei   \n",
       "23331  Eu adoro roupa, adoro moda.. Mas trabalhar num...  detesto OR detestei   \n",
       "\n",
       "         target                                        clean_tweet  \n",
       "0      positive                             tav tao apart ach golp  \n",
       "1      positive                            bom dia sweti ! sab ! ‚òï  \n",
       "2      positive                                  bom dia sab tod ‚úå  \n",
       "3      positive                               tao hand merec tud !  \n",
       "4      positive                        tao final castel branc xuxu  \n",
       "...         ...                                                ...  \n",
       "23327  negative  music als doi minut episodi music anatom grey ...  \n",
       "23328  negative                                           situ poh  \n",
       "23329  negative    lind noit son sonh pesso conhec faculdad ent ta  \n",
       "23330  negative      evit sempr exist altern fac tap ryana ra tem‚Ä¶  \n",
       "23331  negative  ador roup ador mod trabalh loj roup lid pesso ...  \n",
       "\n",
       "[22842 rows x 6 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "outfile = open(\"data/df\", 'wb')\n",
    "pickle.dump(df, outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a document-term matrix using CountVectorizer\n",
    "This produces a scipy sparse matrix with all tweets as rows and all words as columns. It's pretty big; it is essential to keep it as a sparse data structure to avoid memory errors. sklearn can take this kind of objects as input without needing to turn them into dense structures, so that's what I will do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "doc_term_matrix = vectorizer.fit_transform(df[\"clean_tweet\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "outfile = open(\"data/vectorizer\", 'wb')\n",
    "pickle.dump(vectorizer, outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<22842x12569 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 150810 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_term_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "outfile = open(\"data/doc_term_matrix\", 'wb')\n",
    "pickle.dump(doc_term_matrix, outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a pivoted version of the document-term matrix\n",
    "I will still need the scipy one for the models I implement on sklearn, but I also want to implement the Naive Bayes model from scratch. To do this, it is very convenient to produce a pivot table from this document-term matrix. I'm not sure how to do this efficiently with a scipy sparse matrix object, but with pandas it's quite straightforward. Luckily, pandas has a sparse version of dataframes, so I will convert my scipy sparse matrix object to one of those. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ab</th>\n",
       "      <th>abac</th>\n",
       "      <th>abacat</th>\n",
       "      <th>abacax</th>\n",
       "      <th>abaf</th>\n",
       "      <th>abaix</th>\n",
       "      <th>abal</th>\n",
       "      <th>aban</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abat</th>\n",
       "      <th>...</th>\n",
       "      <th>zuk</th>\n",
       "      <th>zulm</th>\n",
       "      <th>zum</th>\n",
       "      <th>zumb</th>\n",
       "      <th>zuzu</th>\n",
       "      <th>‚ÑÇ‚Ñô</th>\n",
       "      <th>‚Ñï‚ÑÇ‚Ñï</th>\n",
       "      <th>‚Ñô‚Ñù</th>\n",
       "      <th>Ïä§Ìä∏Î†àÏù¥ÌÇ§Ï¶à</th>\n",
       "      <th>Ï†úÏù¥ÌÅ¨</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22837</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22838</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22839</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22840</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22841</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22842 rows √ó 12569 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ab  abac  abacat  abacax  abaf  abaix  abal  aban  abandon  abat  ...  \\\n",
       "0       0     0       0       0     0      0     0     0        0     0  ...   \n",
       "1       0     0       0       0     0      0     0     0        0     0  ...   \n",
       "2       0     0       0       0     0      0     0     0        0     0  ...   \n",
       "3       0     0       0       0     0      0     0     0        0     0  ...   \n",
       "4       0     0       0       0     0      0     0     0        0     0  ...   \n",
       "...    ..   ...     ...     ...   ...    ...   ...   ...      ...   ...  ...   \n",
       "22837   0     0       0       0     0      0     0     0        0     0  ...   \n",
       "22838   0     0       0       0     0      0     0     0        0     0  ...   \n",
       "22839   0     0       0       0     0      0     0     0        0     0  ...   \n",
       "22840   0     0       0       0     0      0     0     0        0     0  ...   \n",
       "22841   0     0       0       0     0      0     0     0        0     0  ...   \n",
       "\n",
       "       zuk  zulm  zum  zumb  zuzu  ‚ÑÇ‚Ñô  ‚Ñï‚ÑÇ‚Ñï  ‚Ñô‚Ñù  Ïä§Ìä∏Î†àÏù¥ÌÇ§Ï¶à  Ï†úÏù¥ÌÅ¨  \n",
       "0        0     0    0     0     0   0    0   0       0    0  \n",
       "1        0     0    0     0     0   0    0   0       0    0  \n",
       "2        0     0    0     0     0   0    0   0       0    0  \n",
       "3        0     0    0     0     0   0    0   0       0    0  \n",
       "4        0     0    0     0     0   0    0   0       0    0  \n",
       "...    ...   ...  ...   ...   ...  ..  ...  ..     ...  ...  \n",
       "22837    0     0    0     0     0   0    0   0       0    0  \n",
       "22838    0     0    0     0     0   0    0   0       0    0  \n",
       "22839    0     0    0     0     0   0    0   0       0    0  \n",
       "22840    0     0    0     0     0   0    0   0       0    0  \n",
       "22841    0     0    0     0     0   0    0   0       0    0  \n",
       "\n",
       "[22842 rows x 12569 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_term_matrix = pd.DataFrame.sparse.from_spmatrix(doc_term_matrix, columns = vectorizer.get_feature_names())\n",
    "doc_term_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp/ipykernel_5592/4166249133.py:1: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  doc_term_matrix[\"target_value\"] = np.where(df[\"target\"]==\"positive\", 1, 0)\n"
     ]
    }
   ],
   "source": [
    "doc_term_matrix[\"target_value\"] = np.where(df[\"target\"]==\"positive\", 1, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>target_value</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ab</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abac</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abacat</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abacax</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abaf</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>‚ÑÇ‚Ñô</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>‚Ñï‚ÑÇ‚Ñï</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>‚Ñô‚Ñù</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ïä§Ìä∏Î†àÏù¥ÌÇ§Ï¶à</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ï†úÏù¥ÌÅ¨</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12569 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "target_value  0  1\n",
       "ab            2  3\n",
       "abac          0  1\n",
       "abacat        1  2\n",
       "abacax        0  1\n",
       "abaf          2  1\n",
       "...          .. ..\n",
       "‚ÑÇ‚Ñô            0  1\n",
       "‚Ñï‚ÑÇ‚Ñï           0  1\n",
       "‚Ñô‚Ñù            0  1\n",
       "Ïä§Ìä∏Î†àÏù¥ÌÇ§Ï¶à        0  1\n",
       "Ï†úÏù¥ÌÅ¨           0  1\n",
       "\n",
       "[12569 rows x 2 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pivoted_dcm = doc_term_matrix.pivot_table(columns=[\"target_value\"], aggfunc=\"sum\")\n",
    "pivoted_dcm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "outfile = open(\"data/pivoted_dcm\", 'wb')\n",
    "pickle.dump(pivoted_dcm, outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other/miscellanea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target_value    15786\n",
       "nao              5283\n",
       "dia              1577\n",
       "tod              1346\n",
       "faz              1301\n",
       "tao              1107\n",
       "fic              1038\n",
       "tud               946\n",
       "jog               930\n",
       "pesso             847\n",
       "bom               832\n",
       "ver               817\n",
       "bem               816\n",
       "ano               801\n",
       "quer              790\n",
       "sempr             769\n",
       "pod               766\n",
       "vid               761\n",
       "deu               696\n",
       "vai               681\n",
       "sab               660\n",
       "melhor            649\n",
       "ach               644\n",
       "aind              641\n",
       "hoj               633\n",
       "cois              625\n",
       "ta                621\n",
       "diz               611\n",
       "pas               607\n",
       "agor              585\n",
       "pra               585\n",
       "assim             583\n",
       "outr              575\n",
       "gost              569\n",
       "lind              569\n",
       "nad               548\n",
       "vou               537\n",
       "porqu             530\n",
       "fal               526\n",
       "deix              487\n",
       "vez               475\n",
       "grand             474\n",
       "sei               470\n",
       "sim               467\n",
       "nunc              462\n",
       "cas               461\n",
       "dev               453\n",
       "trabalh           449\n",
       "boa               438\n",
       "dar               437\n",
       "amig              430\n",
       "ganh              410\n",
       "sent              409\n",
       "final             397\n",
       "car               396\n",
       "mim               396\n",
       "aqu               394\n",
       "la                387\n",
       "music             381\n",
       "eurovision        379\n",
       "dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_term_matrix.sum().sort_values(ascending = False)[:60]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing words that are too long to be true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ab</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>abac</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abacat</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abacax</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abaf</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12565</th>\n",
       "      <td>‚Ñï‚ÑÇ‚Ñï</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12566</th>\n",
       "      <td>‚Ñô‚Ñù</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12567</th>\n",
       "      <td>Ïä§Ìä∏Î†àÏù¥ÌÇ§Ï¶à</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12568</th>\n",
       "      <td>Ï†úÏù¥ÌÅ¨</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12569</th>\n",
       "      <td>target_value</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12570 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               word length\n",
       "0                ab      2\n",
       "1              abac      4\n",
       "2            abacat      6\n",
       "3            abacax      6\n",
       "4              abaf      4\n",
       "...             ...    ...\n",
       "12565           ‚Ñï‚ÑÇ‚Ñï      3\n",
       "12566            ‚Ñô‚Ñù      2\n",
       "12567        Ïä§Ìä∏Î†àÏù¥ÌÇ§Ï¶à      6\n",
       "12568           Ï†úÏù¥ÌÅ¨      3\n",
       "12569  target_value     12\n",
       "\n",
       "[12570 rows x 2 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_lengths = pd.DataFrame([doc_term_matrix.columns, [len(i) for i in doc_term_matrix.columns]]).transpose()\n",
    "word_lengths.columns = [\"word\", \"length\"]\n",
    "word_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5     2328\n",
       "4     2266\n",
       "6     2230\n",
       "3     1602\n",
       "7     1489\n",
       "8      928\n",
       "9      531\n",
       "2      361\n",
       "10     301\n",
       "11     145\n",
       "12     115\n",
       "13      70\n",
       "14      45\n",
       "15      33\n",
       "16      29\n",
       "18      22\n",
       "17      19\n",
       "19      13\n",
       "21       7\n",
       "20       7\n",
       "22       6\n",
       "26       5\n",
       "23       5\n",
       "24       4\n",
       "25       2\n",
       "28       2\n",
       "30       1\n",
       "42       1\n",
       "39       1\n",
       "60       1\n",
       "27       1\n",
       "Name: length, dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_lengths[\"length\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>abdelmassih</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>abracadinhos</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>abracadinhosparatodav</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>abracadinhostod</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>aceitamaristel</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>achievement</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>adolescente</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>agarradinhosmuit</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>aguamonchiqu</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>ahahahahaha</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>ahahahahahah</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>ahahahahahaha</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>ahahahahahahah</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>ahahahahahahahah</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>ahahahahahahahahah</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>ahahahahahahahahahah</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>ahahahahahahahahahahah</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313</th>\n",
       "      <td>ahahahahahahahahahahahahahah</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>ahahahahahahahahahahahahahahahahahahahahah</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>ahahahahshjsh</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332</th>\n",
       "      <td>aindavaodizerquefoisort</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338</th>\n",
       "      <td>ajhadkakdsj</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>ajusticasomosno</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>405</th>\n",
       "      <td>alexandrenet</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406</th>\n",
       "      <td>alexandrenetodigitalconsulting</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>alforbuterparty</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>436</th>\n",
       "      <td>alicenopaisdasmaravilh</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>altogethernow</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>508</th>\n",
       "      <td>amaresparasiempr</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>551</th>\n",
       "      <td>amigosparasempr</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>amordeperdica</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>574</th>\n",
       "      <td>amorzitomuit</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>576</th>\n",
       "      <td>amorzitoquer</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577</th>\n",
       "      <td>amorzitoqueridocomt</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>578</th>\n",
       "      <td>amorzitoqueridod</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>614</th>\n",
       "      <td>andebolportug</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>646</th>\n",
       "      <td>aniversario</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>647</th>\n",
       "      <td>aniversariod</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>648</th>\n",
       "      <td>aniversariode</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>649</th>\n",
       "      <td>aniversariomerec</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>650</th>\n",
       "      <td>aniversarioqu</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>651</th>\n",
       "      <td>aniversariotenh</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>692</th>\n",
       "      <td>antoniocost</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>693</th>\n",
       "      <td>antoniodavidfueradelatv</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>694</th>\n",
       "      <td>antoniodnotienehuev</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>702</th>\n",
       "      <td>anygabriely</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>apoieumautornac</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>768</th>\n",
       "      <td>apologizing</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>783</th>\n",
       "      <td>apresentacom</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>843</th>\n",
       "      <td>armyofthedead</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>975</th>\n",
       "      <td>atackontitan</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1002</th>\n",
       "      <td>atletaolimp</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1042</th>\n",
       "      <td>aumalinetecnolog</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1066</th>\n",
       "      <td>autoconfianc</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1067</th>\n",
       "      <td>autocontrol</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1069</th>\n",
       "      <td>autodetermin</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1070</th>\n",
       "      <td>autodeterminacion</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1095</th>\n",
       "      <td>avantipalestr</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1114</th>\n",
       "      <td>aviationacademy</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1115</th>\n",
       "      <td>aviationtraining</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            word length\n",
       "11                                   abdelmassih     11\n",
       "38                                  abracadinhos     12\n",
       "39                         abracadinhosparatodav     21\n",
       "40                               abracadinhostod     15\n",
       "84                                aceitamaristel     14\n",
       "95                                   achievement     11\n",
       "180                                  adolescente     11\n",
       "250                             agarradinhosmuit     16\n",
       "287                                 aguamonchiqu     12\n",
       "305                                  ahahahahaha     11\n",
       "306                                 ahahahahahah     12\n",
       "307                                ahahahahahaha     13\n",
       "308                               ahahahahahahah     14\n",
       "309                             ahahahahahahahah     16\n",
       "310                           ahahahahahahahahah     18\n",
       "311                         ahahahahahahahahahah     20\n",
       "312                       ahahahahahahahahahahah     22\n",
       "313                 ahahahahahahahahahahahahahah     28\n",
       "314   ahahahahahahahahahahahahahahahahahahahahah     42\n",
       "315                                ahahahahshjsh     13\n",
       "332                      aindavaodizerquefoisort     23\n",
       "338                                  ajhadkakdsj     11\n",
       "345                              ajusticasomosno     15\n",
       "405                                 alexandrenet     12\n",
       "406               alexandrenetodigitalconsulting     30\n",
       "412                              alforbuterparty     15\n",
       "436                       alicenopaisdasmaravilh     22\n",
       "473                                altogethernow     13\n",
       "508                             amaresparasiempr     16\n",
       "551                              amigosparasempr     15\n",
       "567                                amordeperdica     13\n",
       "574                                 amorzitomuit     12\n",
       "576                                 amorzitoquer     12\n",
       "577                          amorzitoqueridocomt     19\n",
       "578                             amorzitoqueridod     16\n",
       "614                                andebolportug     13\n",
       "646                                  aniversario     11\n",
       "647                                 aniversariod     12\n",
       "648                                aniversariode     13\n",
       "649                             aniversariomerec     16\n",
       "650                                aniversarioqu     13\n",
       "651                              aniversariotenh     15\n",
       "692                                  antoniocost     11\n",
       "693                      antoniodavidfueradelatv     23\n",
       "694                          antoniodnotienehuev     19\n",
       "702                                  anygabriely     11\n",
       "766                              apoieumautornac     15\n",
       "768                                  apologizing     11\n",
       "783                                 apresentacom     12\n",
       "843                                armyofthedead     13\n",
       "975                                 atackontitan     12\n",
       "1002                                 atletaolimp     11\n",
       "1042                            aumalinetecnolog     16\n",
       "1066                                autoconfianc     12\n",
       "1067                                 autocontrol     11\n",
       "1069                                autodetermin     12\n",
       "1070                           autodeterminacion     17\n",
       "1095                               avantipalestr     13\n",
       "1114                             aviationacademy     15\n",
       "1115                            aviationtraining     16"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_lengths.loc[word_lengths[\"length\"]>10][0:60]\n",
    "#word_lengths.loc[word_lengths[\"length\"]>10][0:60].to_csv(\"data/word_length_sample.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At one point, I took a sample of 60 words over 10 characters. With the addition of new data this sample will likely change so I saved it to data/word_length_sample.csv for future referencing. \n",
    "\n",
    "A threshold of 12 letters seemed to be suitable to get rid of nonsense words. Out of this sample of 60 words roots over 10 characters, 18.3% were meaningful portuguese words that can carry meaning, all of them 12 characters or fewer; no words over 12 characters were found to be meaningful portuguese words.\n",
    "\n",
    "(autodetermin, caleidoscopi, coincidissem, comparticip, concentraca, confidencial, consentimen, conservatori, contraditori, contribuint, contribuint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([   11,    38,    39,    40,    84,    95,   180,   250,   287,\n",
       "              305,\n",
       "            ...\n",
       "            12299, 12333, 12358, 12366, 12392, 12453, 12502, 12508, 12512,\n",
       "            12569],\n",
       "           dtype='int64', length=534)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_lengths.loc[word_lengths[\"length\"]>10].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_term_matrix = doc_term_matrix.iloc[:, word_lengths.loc[word_lengths[\"length\"]<13].index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         6\n",
       "1         5\n",
       "2         5\n",
       "3         5\n",
       "4         6\n",
       "         ..\n",
       "22837    10\n",
       "22838     2\n",
       "22839     9\n",
       "22840     9\n",
       "22841    14\n",
       "Length: 22842, dtype: int64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_words_clean = doc_term_matrix.sum(axis=1)\n",
    "n_words_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Things to do:\n",
    "- add a function to reduce letters repeated more than twice --> DONE\n",
    "- add a function to remove plurals (unnecessary if stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fml might not be a good negative keyword. Turns out, in brazilian portuguese it is also used as a diminutive for familia,\n",
    "#which is used coloquially kind of like fam in US english\n",
    "\n",
    "df.loc[df[\"keyword\"]==\"fml\", \"tweet\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package rslp to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping stemmers\\rslp.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('rslp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "positive    16139\n",
       "negative     7193\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"target\"].value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
